print("hello world")


import requests
from bs4 import BeautifulSoup


about_it_url = 'https://www.nitrr.ac.in/aboutit.php'
about_it_html = requests.get(about_it_url).text
soup = BeautifulSoup(about_it_html, "html.parser")


print("table" in about_it_html)


tables = soup.find_all('table')
for i, table in enumerate(tables, start=1):
    if not table:
        continue
    if 'Faculty' in table.get_text(strip=True):
        print(i)
    rows = table.find_all('tr')
    for row in rows:
        if not row:
            continue
        cells = row.find_all(['td', 'th'])
        if not cells:
            continue
        for cell in cells:
            txt = cell.get_text(strip=True)


def get_faculty_table_indices(tables) -> list:
    res = []
    for i, table in enumerate(tables):
        if not table:
            continue
        if 'Qualification' in table.get_text(strip=True):
            res.append(i)
    return res


def extract_faculty_info(url: str) -> dict:
    html = requests.get(url).text
    soup = BeautifulSoup(html, "html.parser")
    print(f'got data from url {url}')

    tables = soup.find_all('table')
    fac_indices = get_faculty_table_indices(tables)

    if not fac_indices:
        print(f'no faculty table indices found for url {url}')
        return

    print(f'found {len(fac_indices)} facutly tables')

    res = []
    data_columns = []
    for idx in fac_indices:
        table = tables[idx]
        rows = table.find_all('tr')
        if not rows:
            print(f'no rows in table {idx}')
            continue
        found = False
        for row in rows:
            if 'Name' in row.get_text(strip=True):
                found = True
                print(f'found heading column for table {idx}')
                data_columns = [cell.get_text(strip=True) for cell in row.find_all(['td', 'th'])]
                continue
            if not found:
                continue
            cells = row.find_all('td')
            if len(data_columns) != len(cells):
                print(f'len of data_columns: {len(data_columns)} != len of cells ({len(cells)}) of table {idx}. Skipping collection')
                continue
            cur_fac_data = {}
            should_add = True
            for i, col_name in enumerate(data_columns):
                txt = cells[i].get_text(strip=True)
                if not txt:
                    should_add = False
                    break
                cur_fac_data[col_name] = txt
            if should_add:
                res.append(cur_fac_data)

    return res


import json
branches = ['biomed', 'biotech', 'chemical', 'cse', 'electrical', 'electronics', 'it', 'mechanical', 'mining', 'meta']
data_dir = '/home/sagar/python/rag-tutorial/minor-project/data/faculty-data'
for branch in branches:
    url = f'https://nitrr.ac.in/about{branch}.php'
    res = extract_faculty_info(url)
    if not res:
        print(f'failed to extract data for branch {branch}')
        continue
    with open(f'{data_dir}/{branch}.json', 'w+') as file:
        file.write(json.dumps(res))
    print(f'extracted data for branch {branch}')


tables[23]


tables[20]


# find the table
table = soup.find("table", class_="table-borderd")

# extract rows
rows = table.find_all("tr")

# parse each row
for row in rows:
    cells = [cell.get_text(strip=True) for cell in row.find_all(["th", "td"])]
    print(cells)


def extract_faculty_data





































































